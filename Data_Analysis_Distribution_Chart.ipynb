{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snowdenrise/Data_Analysis_Distribution_Chart/blob/main/Data_Analysis_Distribution_Chart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFqSjZUDah-x"
      },
      "source": [
        "*Press Ctrl + F9 if you want to run it all at once*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW98hT1EfSPl"
      },
      "source": [
        "#Run this first! üôÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOjW8oMKhxml"
      },
      "outputs": [],
      "source": [
        "#Refactoring coding - Jonatan Serna\n",
        "\n",
        "\n",
        "\n",
        "#Importing Libraries\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import scipy as sc\n",
        "import pandas as pd\n",
        "from scipy.stats import nbinom, poisson, geom, norm\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "#Fitting functions\n",
        "\n",
        "def mean_confidence_interval(data, confidence=0.95):\n",
        "      a = 1.0 * np.array(data)\n",
        "      n = len(a)\n",
        "      m, se = np.mean(a), sc.stats.sem(a)\n",
        "      h = se * sc.stats.t.ppf((1 + confidence) / 2., n-1)\n",
        "      return m, m-h, m+h\n",
        "\n",
        "def fit_continuous(data,discrete=False):\n",
        "  if(discrete):\n",
        "    y,x = np.histogram(data,density=True)\n",
        "    x = (x+np.roll(x,-1))[:-1]/2.0\n",
        "\n",
        "    best_distributions=[]\n",
        "    distributions = ['norm','t']\n",
        "\n",
        "    try:\n",
        "      with warnings.catch_warnings():\n",
        "        warnings.filterwarnings('ignore')\n",
        "        for distribution in distributions:\n",
        "          dist = getattr(sc.stats,distribution)\n",
        "          params = dist.fit(data)\n",
        "\n",
        "\n",
        "          #Separating the parameters\n",
        "          params_sep=params[:-2]\n",
        "          loc=params[-2]\n",
        "          scale=params[-1]\n",
        "\n",
        "          #PDF and error\n",
        "\n",
        "          pdf=dist.pdf(x,loc=loc,scale=scale, *params_sep)\n",
        "          sse = np.sum(np.power(y-pdf,2.0))\n",
        "          \n",
        "\n",
        "          best_distributions.append((distribution,list(params),sse))\n",
        "    except Exceptions:\n",
        "      pass\n",
        "    \n",
        "    sorted_best_dist=sorted(best_distributions,key=lambda x:x[2])\n",
        "    if(sorted_best_dist[0][2]<=0.2):\n",
        "      data_stt = pd.Series(data)\n",
        "      print(\"Dataset Information\")\n",
        "      print(\"Mean: \",data_stt.mean())\n",
        "      print(\"Sd: \",np.sqrt(data_stt.var()))\n",
        "\n",
        "      print(\"Fit Information\")\n",
        "      print(\"Best Fit: \",sorted_best_dist[0][0])\n",
        "      print(\"Best Fit params: \",sorted_best_dist[0][1])\n",
        "      print(\"Best Fit SSE: \",sorted_best_dist[0][2])\n",
        "      interval = mean_confidence_interval(data)\n",
        "      print(\"Mean Confidence Interval (95%)\")\n",
        "      print(\"[\",interval[1],\";\", interval[2],\"]\")\n",
        "\n",
        "      best_dist=getattr(sc.stats,sorted_best_dist[0][0])\n",
        "      best_params=sorted_best_dist[0][1]\n",
        "\n",
        "      # Separate best parameters\n",
        "      best_arg = best_params[:-2]\n",
        "      best_loc = best_params[-2]\n",
        "      best_scale = best_params[-1]\n",
        "\n",
        "      # Get sane start and end points of distribution\n",
        "      best_start = best_dist.ppf(0.01, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.01, loc=best_loc, scale=best_scale)\n",
        "      best_end = best_dist.ppf(0.99, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.99, loc=best_loc, scale=best_scale)\n",
        "\n",
        "      # Build PDF and turn into pandas Series\n",
        "      best_x = np.linspace(best_start, best_end, num=10000)\n",
        "      best_y = best_dist.pdf(best_x,loc=best_loc, scale=best_scale, *best_arg)\n",
        "      best_pdf = pd.Series(best_y, best_x)\n",
        "\n",
        "      plt.figure(figsize=(8,8))\n",
        "      plt.title('Best distribution \" '+sorted_best_dist[0][0]+' \"')\n",
        "      plt.hist(data,density=True)\n",
        "      best_pdf.plot(lw=2,label=\"Best Fit\",legend=True)\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  else:\n",
        "    data_stt = pd.Series(data)\n",
        "    print(\"Dataset Information\")\n",
        "    print(\"Mean: \",data_stt.mean())\n",
        "    print(\"Sd: \",np.sqrt(data_stt.var()))\n",
        "\n",
        "    y,x = np.histogram(data,density=True)\n",
        "    x = (x+np.roll(x,-1))[:-1]/2.0\n",
        "\n",
        "    best_distributions=[]\n",
        "    distributions = ['norm','t','expon','gamma']\n",
        "\n",
        "    try:\n",
        "      with warnings.catch_warnings():\n",
        "        warnings.filterwarnings('ignore')\n",
        "        for distribution in distributions:\n",
        "          dist = getattr(sc.stats,distribution)\n",
        "          params = dist.fit(data)\n",
        "\n",
        "\n",
        "          #Separating the parameters\n",
        "          params_sep=params[:-2]\n",
        "          loc=params[-2]\n",
        "          scale=params[-1]\n",
        "\n",
        "          #PDF and error\n",
        "\n",
        "          pdf=dist.pdf(x,loc=loc,scale=scale, *params_sep)\n",
        "          sse = np.sum(np.power(y-pdf,2.0))\n",
        "\n",
        "          best_distributions.append((distribution,list(params),sse))\n",
        "    except Exceptions:\n",
        "      pass\n",
        "\n",
        "    sorted_best_dist=sorted(best_distributions,key=lambda x:x[2])\n",
        "    print(\"Fit Information\")\n",
        "    print(\"Best Fit: \",sorted_best_dist[0][0])\n",
        "    print(\"Best Fit params: \",sorted_best_dist[0][1])\n",
        "    print(\"Best Fit SSE: \",sorted_best_dist[0][2])\n",
        "\n",
        "    best_dist=getattr(sc.stats,sorted_best_dist[0][0])\n",
        "    best_params=sorted_best_dist[0][1]\n",
        "\n",
        "    # Separate best parameters\n",
        "    best_arg = best_params[:-2]\n",
        "    best_loc = best_params[-2]\n",
        "    best_scale = best_params[-1]\n",
        "\n",
        "    # Get sane start and end points of distribution\n",
        "    best_start = best_dist.ppf(0.01, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.01, loc=best_loc, scale=best_scale)\n",
        "    best_end = best_dist.ppf(0.99, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.99, loc=best_loc, scale=best_scale)\n",
        "\n",
        "    # Build PDF and turn into pandas Series\n",
        "    best_x = np.linspace(best_start, best_end, num=10000)\n",
        "    best_y = best_dist.pdf(best_x,loc=best_loc, scale=best_scale, *best_arg)\n",
        "    best_pdf = pd.Series(best_y, best_x)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.title('Best distribution \" '+sorted_best_dist[0][0]+' \"')\n",
        "    plt.hist(data,density=True)\n",
        "    best_pdf.plot(lw=2,label=\"Best Fit\",legend=True)\n",
        "    interval = mean_confidence_interval(data)\n",
        "    print(\"Mean Confidence Interval (95%)\")\n",
        "    print(\"[\",interval[1],\";\", interval[2],\"]\")\n",
        "\n",
        "\n",
        "def fit_discrete(data,ignore=False):\n",
        "  \n",
        "  if(ignore):\n",
        "    data_try=False\n",
        "  else:\n",
        "    data_try=fit_continuous(data,discrete=True)\n",
        "\n",
        "  if (data_try==False):\n",
        "    n=len(data)\n",
        "    data_fit=pd.Series(data)\n",
        "    print(\"Dataset Information\")\n",
        "\n",
        "    mean = data_fit.mean()\n",
        "    var = data_fit.var()\n",
        "\n",
        "    print(\"Mean: \",mean)\n",
        "    print(\"Sd: \",np.sqrt(var))\n",
        "\n",
        "    likelihoods = {}\n",
        "    params = {}\n",
        "    try:\n",
        "      with warnings.catch_warnings():\n",
        "        warnings.filterwarnings('ignore')\n",
        "        #Negative Binomial distribution\n",
        "\n",
        "        p_nb = mean/var\n",
        "        r_nb = p_nb*mean / (1-p_nb) \n",
        "\n",
        "        likelihoods['nbinomial'] = np.sum(np.log(np.array(data_fit.map(lambda val: nbinom.pmf(val, r_nb, p_nb)))))\n",
        "        params[\"nbinomial\"] = [r_nb,p_nb]\n",
        "\n",
        "        #Poisson Distribution\n",
        "\n",
        "        lambda_poisson = mean\n",
        "\n",
        "        likelihoods['poisson'] = np.sum(np.log(np.array(data_fit.map(lambda val: poisson.pmf(val, lambda_poisson)))))\n",
        "        params[\"poisson\"] = lambda_poisson\n",
        "\n",
        "        #Geometric Distribution\n",
        "\n",
        "        p_geo=1/mean\n",
        "\n",
        "        likelihoods['geometric'] = np.sum(np.log(np.array(data_fit.map(lambda val: geom.pmf(val, p_geo)))))\n",
        "        params[\"geometric\"] = p_geo\n",
        "  \n",
        "    except Exception:\n",
        "      pass\n",
        "\n",
        "    #Start printing the information\n",
        "    \n",
        "    likelihoods = {k: likelihoods[k] for k in likelihoods if not np.isnan(likelihoods[k])}\n",
        "\n",
        "    best_fit = max(likelihoods, key=lambda x: likelihoods[x])\n",
        "    print(\"Best Fit Information\")\n",
        "    print(\"Best fit:\", best_fit)\n",
        "    print(\"Log Likelihood:\", likelihoods[best_fit])\n",
        "    print(\"Parameters:\", params[best_fit])\n",
        "\n",
        "    interval = mean_confidence_interval(data)\n",
        "    print(\"Mean Confidence Interval (95%)\")\n",
        "    print(\"[\",interval[1],\";\", interval[2],\"]\")\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "\n",
        "    plt.hist(data,20,density=True)\n",
        "    k = np.arange(data_fit.min(),data_fit.max()+1)\n",
        "    plt.plot(k,poisson.pmf(k,lambda_poisson),'go', markersize=5,label=\"Poisson\")\n",
        "    plt.plot(k,nbinom.pmf(k,r_nb,p_nb),'go', markersize=5,color=\"red\",label=\"Negative Binomial\")\n",
        "    plt.plot(k,geom.pmf(k,p_geo),'go', markersize=5,color=\"orange\",label=\"Geometric\")\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "def fit_distribution(data_name,column, continuous=False,ignore=False):\n",
        "#Assigning uploaded CSV to the dataframe variable\n",
        "  data_f=pd.read_csv(data_name)\n",
        "#Changing Columns to Upper Case\n",
        "  column = column.upper()\n",
        "  data_f.columns = data_f.columns.str.upper()\n",
        "\n",
        "  data=data_f[column]\n",
        "  if(continuous):\n",
        "    fit_continuous(data)\n",
        "  else:\n",
        "    fit_discrete(data,ignore)\n",
        "\n",
        "def fit_distribution_test(data, continuous=False,ignore=False):\n",
        "  if(continuous):\n",
        "    fit_continuous(data)\n",
        "  else:\n",
        "    fit_discrete(data,ignore)\n",
        "\n",
        "#Remove below # in the comments if you want to test the code\n",
        "#print(\"Test Below ............ /n\")\n",
        "#data_1= [12,12,12,12,13,13,13,13,15,14,14,14,14,14,15,15,15,15,15,15,16,16,16,16,18,18,18,18,17,17,17,17,17,17,]\n",
        "#fit_distribution_test(data_1)\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------------------------------------------\n",
        "#Debes subir el dataset al mismo directorio, si estas desde Google Colab, Deberia ser en /content/\n",
        "#debe tener las columnas, Date, Employee, y el dato totalizado por fecha, en este ejemplo, Bookings\n",
        "#Guardar como CSV\n",
        "#Se deben correr las primeras lineas para cargar los modulos y librerias\n",
        "\n",
        "dataset = \"\"\n",
        "columnAnalyze = \"\"\n",
        "datasetOk = \"\"\n",
        "\n",
        "def printingColumns():\n",
        "  def extract_columns(df_index):\n",
        "      # Convertir el objeto Index en una lista de Python\n",
        "      column_list = df_index.tolist()\n",
        "\n",
        "      # Procesar cada elemento de la lista para extraer el nombre de la columna\n",
        "      column_names = []\n",
        "      for col in column_list:\n",
        "          col_name = col.replace('\\nAvailable columns: ', '')\n",
        "          column_names.append(col_name)\n",
        "\n",
        "      return column_names\n",
        "  # Crear el objeto Index\n",
        "  index = pd.Index(['\\nAvailable columns: Week', '\\nAvailable columns: Year',\n",
        "        '\\nAvailable columns: Date', '\\nAvailable columns: Bookings',\n",
        "        '\\nAvailable columns: Profit', '\\nAvailable columns: Profit %'],\n",
        "        dtype='object')\n",
        "\n",
        "  # Llamar a la funci√≥n extract_columns()\n",
        "  column_names = extract_columns(index)\n",
        "\n",
        "  # Imprimir los nombres de las columnas\n",
        "  print(\"\\nAvailable columns:\\n\")\n",
        "  print(column_names)\n",
        "  return\n",
        "\n",
        "def runningTheBellCurve():\n",
        "  datasetOk = input(\"\\n\\nHave you uploaded the dataset already?, YES or NO  ü§î>>>>>>        \\n\")\n",
        "  datasetOk = datasetOk.lower()\n",
        "  if (datasetOk == \"yes\" or datasetOk == \"y\" or datasetOk == \"si\"):\n",
        "    print(\"\\n------------------------------------------------\\nNice üëç, thanks, now:  \\n\")\n",
        "    dataset = input(\"\\n\\nWhat is the full name (including the extension), of the dataset, make sure you save it in the same folder (possible '/content/' if you are using Google Colab) as a CSV File üìÇ  >>>>>>      \\n\") \n",
        "    printingColumns()\n",
        "    columnAnalyze = input(\"\\n\\n What is the name of the column to analyze üìä ?, please enter the SAME name, this is Case Sensitive üî†üî° >>>>>>      \\n\\n\") \n",
        "    print(\"\\n------------------------------------------------\\n#Running üöÄ\\n------------------------------------------------\\nDistribution Chart\\n ‚è¨         ‚è¨         ‚è¨\\n------------------------------------------------\\n\")\n",
        "    fit_distribution(dataset,columnAnalyze)\n",
        "  else:\n",
        "    print(\"\\n\\n Please upload the dataset as a CSV File, \\n\\n Preferably, name it as data.csv, it would be easier for both of us üòä \\n\\n\")\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "      print(\"------------------------------------------------------------------------------\")\n",
        "      print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "      print(\"------------------------------------------------------------------------------\")\n",
        "      dataset = '{name}'.format(name=fn) #Assigning the name of the just uploaded document\n",
        "    #Assigning (temporary) uploaded CSV to the dataframe variable\n",
        "    data_f=pd.read_csv(dataset)\n",
        "    #print(\"\\nAvailable columns: \"+data_f.columns) #Printing Columns\n",
        "    printingColumns()\n",
        "    columnAnalyze = input(\"\\n\\n What is the name of the column to analyze üìä ?, please enter the SAME name, this is Case Sensitive üî†üî° >>>>>>      \\n \") \n",
        "    print(\"\\n------------------------------------------------\\n#Running üöÄ\\n------------------------------------------------\\nDistribution Chart\\n ‚è¨         ‚è¨         ‚è¨\\n------------------------------------------------\\n\")\n",
        "    fit_distribution(dataset,columnAnalyze)\n",
        "  return\n",
        "\n",
        "\n",
        "#Running the Bell Curve NOW!!!\n",
        "\n",
        "#runningTheBellCurve()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGLVkfyE2LqF"
      },
      "source": [
        "#Now run this üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z0c7nxsE58VC",
        "outputId": "1d00bd3e-a059-47d8-b8b0-0c34299da70f"
      },
      "outputs": [],
      "source": [
        "runningTheBellCurve()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Manual visual with Seaborn üåä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "import warnings\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "df_train = pd.read_csv(\"data_loadsmith.csv\")\n",
        "\n",
        "sns.distplot(df_train[\"Bookings\"]) ##Distribution Plot\n",
        "\n",
        "#sns.set(font_scale=1.25) \n",
        "\n",
        "#    *hm = sns.heatmap(cm, cbar=True, annot=True, fmt=\".2f\", annot_kws={\"size\":10}, yticklabels=cols.values, xticklabels=cols.values)\n",
        "#   plt.show()**/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Refactoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Refactoring coding - Jonatan Serna\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Define functions\n",
        "def mean_confidence_interval(data, confidence=0.95):\n",
        "    a = 1.0 * np.array(data)\n",
        "    n = len(a)\n",
        "    m, se = np.mean(a), stats.sem(a)\n",
        "    h = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
        "    return m, m-h, m+h\n",
        "\n",
        "def fit_continuous(data, discrete=False):\n",
        "    # Get dataset statistics\n",
        "    data_stt = pd.Series(data)\n",
        "    print(\"Dataset Information\")\n",
        "    print(\"Mean: \", data_stt.mean())\n",
        "    print(\"Sd: \", np.sqrt(data_stt.var()))\n",
        "\n",
        "    # Create histogram\n",
        "    y, x = np.histogram(data, density=True)\n",
        "    x = (x + np.roll(x, -1))[:-1] / 2.0\n",
        "\n",
        "    # Fit distributions and find best one\n",
        "    best_distributions = []\n",
        "    distributions = ['norm', 't', 'expon', 'gamma'] if not discrete else ['norm', 't']\n",
        "\n",
        "    try:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings('ignore')\n",
        "            for distribution in distributions:\n",
        "                dist = getattr(stats, distribution)\n",
        "                params = dist.fit(data)\n",
        "\n",
        "                #Separating the parameters\n",
        "                params_sep = params[:-2]\n",
        "                loc = params[-2]\n",
        "                scale = params[-1]\n",
        "\n",
        "                # Calculate PDF and error\n",
        "                pdf = dist.pdf(x, loc=loc, scale=scale, *params_sep)\n",
        "                sse = np.sum(np.power(y - pdf, 2.0))\n",
        "\n",
        "                best_distributions.append((distribution, list(params), sse))\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return False\n",
        "\n",
        "    sorted_best_dist = sorted(best_distributions, key=lambda x: x[2])\n",
        "    print(\"Fit Information\")\n",
        "    print(\"Best Fit: \", sorted_best_dist[0][0])\n",
        "    print(\"Best Fit params: \", sorted_best_dist[0][1])\n",
        "    print(\"Best Fit SSE: \", sorted_best_dist[0][2])\n",
        "\n",
        "    # Plot histogram and best fit\n",
        "    best_dist = getattr(stats, sorted_best_dist[0][0])\n",
        "    best_params = sorted_best_dist[0][1]\n",
        "\n",
        "    # Separate best parameters\n",
        "    best_arg = best_params[:-2]\n",
        "    best_loc = best_params[-2]\n",
        "    best_scale = best_params[-1]\n",
        "\n",
        "    # Get sane start and end points of distribution\n",
        "    best_start = best_dist.ppf(0.01, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.01, loc=best_loc, scale=best_scale)\n",
        "    best_end = best_dist.ppf(0.99, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.99, loc=best_loc, scale=best_scale)\n",
        "\n",
        "    # Build PDF and turn into pandas Series\n",
        "    best_x = np.linspace(best_start, best_end, num=10000)\n",
        "    best_y = best_dist.pdf(best_x, loc=best_loc, scale=best_scale, *best_arg)\n",
        "    best_pdf = pd.Series(best_y, best_x)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.title('Best distribution \"' + sorted_best_dist[0][0] + '\"')\n",
        "    plt\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "J6wpL84HuX3Z"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "515ab5237b3b2b704a65bb16665df991990f8d9b97e10d580243abdf3d748bec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
