{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snowdenrise/Data_Analysis_Distribution_Chart/blob/main/Data_Analysis_Distribution_Chart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFqSjZUDah-x"
      },
      "source": [
        "*Press Ctrl + F9 if you want to run it all at once*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW98hT1EfSPl"
      },
      "source": [
        "#Run this first! ðŸ™‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOjW8oMKhxml"
      },
      "outputs": [],
      "source": [
        "#Refactoring coding - Jonatan Serna\n",
        "\n",
        "\n",
        "\n",
        "#Importing Libraries\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import scipy as sc\n",
        "import pandas as pd\n",
        "from scipy.stats import nbinom, poisson, geom, norm\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "#Fitting functions\n",
        "\n",
        "def mean_confidence_interval(data, confidence=0.95):\n",
        "      a = 1.0 * np.array(data)\n",
        "      n = len(a)\n",
        "      m, se = np.mean(a), sc.stats.sem(a)\n",
        "      h = se * sc.stats.t.ppf((1 + confidence) / 2., n-1)\n",
        "      return m, m-h, m+h\n",
        "\n",
        "def fit_continuous(data,discrete=False):\n",
        "  if(discrete):\n",
        "    y,x = np.histogram(data,density=True)\n",
        "    x = (x+np.roll(x,-1))[:-1]/2.0\n",
        "\n",
        "    best_distributions=[]\n",
        "    distributions = ['norm','t']\n",
        "\n",
        "    try:\n",
        "      with warnings.catch_warnings():\n",
        "        warnings.filterwarnings('ignore')\n",
        "        for distribution in distributions:\n",
        "          dist = getattr(sc.stats,distribution)\n",
        "          params = dist.fit(data)\n",
        "\n",
        "\n",
        "          #Separating the parameters\n",
        "          params_sep=params[:-2]\n",
        "          loc=params[-2]\n",
        "          scale=params[-1]\n",
        "\n",
        "          #PDF and error\n",
        "\n",
        "          pdf=dist.pdf(x,loc=loc,scale=scale, *params_sep)\n",
        "          sse = np.sum(np.power(y-pdf,2.0))\n",
        "          \n",
        "\n",
        "          best_distributions.append((distribution,list(params),sse))\n",
        "    except Exceptions:\n",
        "      pass\n",
        "    \n",
        "    sorted_best_dist=sorted(best_distributions,key=lambda x:x[2])\n",
        "    if(sorted_best_dist[0][2]<=0.2):\n",
        "      data_stt = pd.Series(data)\n",
        "      print(\"Dataset Information\")\n",
        "      print(\"Mean: \",data_stt.mean())\n",
        "      print(\"Sd: \",np.sqrt(data_stt.var()))\n",
        "\n",
        "      print(\"Fit Information\")\n",
        "      print(\"Best Fit: \",sorted_best_dist[0][0])\n",
        "      print(\"Best Fit params: \",sorted_best_dist[0][1])\n",
        "      print(\"Best Fit SSE: \",sorted_best_dist[0][2])\n",
        "      interval = mean_confidence_interval(data)\n",
        "      print(\"Mean Confidence Interval (95%)\")\n",
        "      print(\"[\",interval[1],\";\", interval[2],\"]\")\n",
        "\n",
        "      best_dist=getattr(sc.stats,sorted_best_dist[0][0])\n",
        "      best_params=sorted_best_dist[0][1]\n",
        "\n",
        "      # Separate best parameters\n",
        "      best_arg = best_params[:-2]\n",
        "      best_loc = best_params[-2]\n",
        "      best_scale = best_params[-1]\n",
        "\n",
        "      # Get sane start and end points of distribution\n",
        "      best_start = best_dist.ppf(0.01, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.01, loc=best_loc, scale=best_scale)\n",
        "      best_end = best_dist.ppf(0.99, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.99, loc=best_loc, scale=best_scale)\n",
        "\n",
        "      # Build PDF and turn into pandas Series\n",
        "      best_x = np.linspace(best_start, best_end, num=10000)\n",
        "      best_y = best_dist.pdf(best_x,loc=best_loc, scale=best_scale, *best_arg)\n",
        "      best_pdf = pd.Series(best_y, best_x)\n",
        "\n",
        "      plt.figure(figsize=(8,8))\n",
        "      plt.title('Best distribution \" '+sorted_best_dist[0][0]+' \"')\n",
        "      plt.hist(data,density=True)\n",
        "      best_pdf.plot(lw=2,label=\"Best Fit\",legend=True)\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  else:\n",
        "    data_stt = pd.Series(data)\n",
        "    print(\"Dataset Information\")\n",
        "    print(\"Mean: \",data_stt.mean())\n",
        "    print(\"Sd: \",np.sqrt(data_stt.var()))\n",
        "\n",
        "    y,x = np.histogram(data,density=True)\n",
        "    x = (x+np.roll(x,-1))[:-1]/2.0\n",
        "\n",
        "    best_distributions=[]\n",
        "    distributions = ['norm','t','expon','gamma']\n",
        "\n",
        "    try:\n",
        "      with warnings.catch_warnings():\n",
        "        warnings.filterwarnings('ignore')\n",
        "        for distribution in distributions:\n",
        "          dist = getattr(sc.stats,distribution)\n",
        "          params = dist.fit(data)\n",
        "\n",
        "\n",
        "          #Separating the parameters\n",
        "          params_sep=params[:-2]\n",
        "          loc=params[-2]\n",
        "          scale=params[-1]\n",
        "\n",
        "          #PDF and error\n",
        "\n",
        "          pdf=dist.pdf(x,loc=loc,scale=scale, *params_sep)\n",
        "          sse = np.sum(np.power(y-pdf,2.0))\n",
        "\n",
        "          best_distributions.append((distribution,list(params),sse))\n",
        "    except Exceptions:\n",
        "      pass\n",
        "\n",
        "    sorted_best_dist=sorted(best_distributions,key=lambda x:x[2])\n",
        "    print(\"Fit Information\")\n",
        "    print(\"Best Fit: \",sorted_best_dist[0][0])\n",
        "    print(\"Best Fit params: \",sorted_best_dist[0][1])\n",
        "    print(\"Best Fit SSE: \",sorted_best_dist[0][2])\n",
        "\n",
        "    best_dist=getattr(sc.stats,sorted_best_dist[0][0])\n",
        "    best_params=sorted_best_dist[0][1]\n",
        "\n",
        "    # Separate best parameters\n",
        "    best_arg = best_params[:-2]\n",
        "    best_loc = best_params[-2]\n",
        "    best_scale = best_params[-1]\n",
        "\n",
        "    # Get sane start and end points of distribution\n",
        "    best_start = best_dist.ppf(0.01, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.01, loc=best_loc, scale=best_scale)\n",
        "    best_end = best_dist.ppf(0.99, *best_arg, loc=best_loc, scale=best_scale) if best_arg else best_dist.ppf(0.99, loc=best_loc, scale=best_scale)\n",
        "\n",
        "    # Build PDF and turn into pandas Series\n",
        "    best_x = np.linspace(best_start, best_end, num=10000)\n",
        "    best_y = best_dist.pdf(best_x,loc=best_loc, scale=best_scale, *best_arg)\n",
        "    best_pdf = pd.Series(best_y, best_x)\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.title('Best distribution \" '+sorted_best_dist[0][0]+' \"')\n",
        "    plt.hist(data,density=True)\n",
        "    best_pdf.plot(lw=2,label=\"Best Fit\",legend=True)\n",
        "    interval = mean_confidence_interval(data)\n",
        "    print(\"Mean Confidence Interval (95%)\")\n",
        "    print(\"[\",interval[1],\";\", interval[2],\"]\")\n",
        "\n",
        "\n",
        "def fit_discrete(data,ignore=False):\n",
        "  \n",
        "  if(ignore):\n",
        "    data_try=False\n",
        "  else:\n",
        "    data_try=fit_continuous(data,discrete=True)\n",
        "\n",
        "  if (data_try==False):\n",
        "    n=len(data)\n",
        "    data_fit=pd.Series(data)\n",
        "    print(\"Dataset Information\")\n",
        "\n",
        "    mean = data_fit.mean()\n",
        "    var = data_fit.var()\n",
        "\n",
        "    print(\"Mean: \",mean)\n",
        "    print(\"Sd: \",np.sqrt(var))\n",
        "\n",
        "    likelihoods = {}\n",
        "    params = {}\n",
        "    try:\n",
        "      with warnings.catch_warnings():\n",
        "        warnings.filterwarnings('ignore')\n",
        "        #Negative Binomial distribution\n",
        "\n",
        "        p_nb = mean/var\n",
        "        r_nb = p_nb*mean / (1-p_nb) \n",
        "\n",
        "        likelihoods['nbinomial'] = np.sum(np.log(np.array(data_fit.map(lambda val: nbinom.pmf(val, r_nb, p_nb)))))\n",
        "        params[\"nbinomial\"] = [r_nb,p_nb]\n",
        "\n",
        "        #Poisson Distribution\n",
        "\n",
        "        lambda_poisson = mean\n",
        "\n",
        "        likelihoods['poisson'] = np.sum(np.log(np.array(data_fit.map(lambda val: poisson.pmf(val, lambda_poisson)))))\n",
        "        params[\"poisson\"] = lambda_poisson\n",
        "\n",
        "        #Geometric Distribution\n",
        "\n",
        "        p_geo=1/mean\n",
        "\n",
        "        likelihoods['geometric'] = np.sum(np.log(np.array(data_fit.map(lambda val: geom.pmf(val, p_geo)))))\n",
        "        params[\"geometric\"] = p_geo\n",
        "  \n",
        "    except Exception:\n",
        "      pass\n",
        "\n",
        "    #Start printing the information\n",
        "    \n",
        "    likelihoods = {k: likelihoods[k] for k in likelihoods if not np.isnan(likelihoods[k])}\n",
        "\n",
        "    best_fit = max(likelihoods, key=lambda x: likelihoods[x])\n",
        "    print(\"Best Fit Information\")\n",
        "    print(\"Best fit:\", best_fit)\n",
        "    print(\"Log Likelihood:\", likelihoods[best_fit])\n",
        "    print(\"Parameters:\", params[best_fit])\n",
        "\n",
        "    interval = mean_confidence_interval(data)\n",
        "    print(\"Mean Confidence Interval (95%)\")\n",
        "    print(\"[\",interval[1],\";\", interval[2],\"]\")\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    plt.figure(figsize=(8,8))\n",
        "\n",
        "    plt.hist(data,20,density=True)\n",
        "    k = np.arange(data_fit.min(),data_fit.max()+1)\n",
        "    plt.plot(k,poisson.pmf(k,lambda_poisson),'go', markersize=5,label=\"Poisson\")\n",
        "    plt.plot(k,nbinom.pmf(k,r_nb,p_nb),'go', markersize=5,color=\"red\",label=\"Negative Binomial\")\n",
        "    plt.plot(k,geom.pmf(k,p_geo),'go', markersize=5,color=\"orange\",label=\"Geometric\")\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "def fit_distribution(data_name,column, continuous=False,ignore=False):\n",
        "#Assigning uploaded CSV to the dataframe variable\n",
        "  data_f=pd.read_csv(data_name)\n",
        "#Changing Columns to Upper Case\n",
        "  column = column.upper()\n",
        "  data_f.columns = data_f.columns.str.upper()\n",
        "\n",
        "  data=data_f[column]\n",
        "  if(continuous):\n",
        "    fit_continuous(data)\n",
        "  else:\n",
        "    fit_discrete(data,ignore)\n",
        "\n",
        "def fit_distribution_test(data, continuous=False,ignore=False):\n",
        "  if(continuous):\n",
        "    fit_continuous(data)\n",
        "  else:\n",
        "    fit_discrete(data,ignore)\n",
        "\n",
        "dataset = \"\"\n",
        "columnAnalyze = \"\"\n",
        "datasetOk = \"\"\n",
        "\n",
        "def printingColumns():\n",
        "  def extract_columns(df_index):\n",
        "      # Convertir el objeto Index en una lista de Python\n",
        "      column_list = df_index.tolist()\n",
        "\n",
        "      # Procesar cada elemento de la lista para extraer el nombre de la columna\n",
        "      column_names = []\n",
        "      for col in column_list:\n",
        "          col_name = col.replace('\\nAvailable columns: ', '')\n",
        "          column_names.append(col_name)\n",
        "\n",
        "      return column_names\n",
        "  # Crear el objeto Index\n",
        "  index = pd.Index(['\\nAvailable columns: Week', '\\nAvailable columns: Year',\n",
        "        '\\nAvailable columns: Date', '\\nAvailable columns: Bookings',\n",
        "        '\\nAvailable columns: Profit', '\\nAvailable columns: Profit %'],\n",
        "        dtype='object')\n",
        "\n",
        "  # Llamar a la funciÃ³n extract_columns()\n",
        "  column_names = extract_columns(index)\n",
        "\n",
        "  # Imprimir los nombres de las columnas\n",
        "  print(\"\\nAvailable columns:\\n\")\n",
        "  print(column_names)\n",
        "  return\n",
        "\n",
        "def runningTheBellCurve():\n",
        "  datasetOk = input(\"\\n\\nHave you uploaded the dataset already?, YES or NO  ðŸ¤”>>>>>>        \\n\")\n",
        "  datasetOk = datasetOk.lower()\n",
        "  if (datasetOk == \"yes\" or datasetOk == \"y\" or datasetOk == \"si\"):\n",
        "    print(\"\\n------------------------------------------------\\nNice ðŸ‘, thanks, now:  \\n\")\n",
        "    dataset = input(\"\\n\\nWhat is the full name (including the extension), of the dataset, make sure you save it in the same folder (possible '/content/' if you are using Google Colab) as a CSV File ðŸ“‚  >>>>>>      \\n\") \n",
        "    printingColumns()\n",
        "    columnAnalyze = input(\"\\n\\n What is the name of the column to analyze ðŸ“Š ?, please enter the SAME name, this is Case Sensitive ðŸ” ðŸ”¡ >>>>>>      \\n\\n\") \n",
        "    print(\"\\n------------------------------------------------\\n#Running ðŸš€\\n------------------------------------------------\\nDistribution Chart\\n â¬         â¬         â¬\\n------------------------------------------------\\n\")\n",
        "    fit_distribution(dataset,columnAnalyze)\n",
        "  else:\n",
        "    print(\"\\n\\n Please upload the dataset as a CSV File, \\n\\n Preferably, name it as data.csv, it would be easier for both of us ðŸ˜Š \\n\\n\")\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "      print(\"------------------------------------------------------------------------------\")\n",
        "      print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "      print(\"------------------------------------------------------------------------------\")\n",
        "      dataset = '{name}'.format(name=fn) #Assigning the name of the just uploaded document\n",
        "    #Assigning (temporary) uploaded CSV to the dataframe variable\n",
        "    data_f=pd.read_csv(dataset)\n",
        "    #print(\"\\nAvailable columns: \"+data_f.columns) #Printing Columns\n",
        "    printingColumns()\n",
        "    columnAnalyze = input(\"\\n\\n What is the name of the column to analyze ðŸ“Š ?, please enter the SAME name, this is Case Sensitive ðŸ” ðŸ”¡ >>>>>>      \\n \") \n",
        "    print(\"\\n------------------------------------------------\\n#Running ðŸš€\\n------------------------------------------------\\nDistribution Chart\\n â¬         â¬         â¬\\n------------------------------------------------\\n\")\n",
        "    fit_distribution(dataset,columnAnalyze)\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGLVkfyE2LqF"
      },
      "source": [
        "#Now run this ðŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z0c7nxsE58VC",
        "outputId": "1d00bd3e-a059-47d8-b8b0-0c34299da70f"
      },
      "outputs": [],
      "source": [
        "runningTheBellCurve()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Manual visual with Seaborn ðŸŒŠ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "import warnings\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
        "\n",
        "df_train = pd.read_csv(\"data_loadsmith.csv\")\n",
        "\n",
        "\n",
        "sns.distplot(data_f[columnAnalyze]) ##Distribution Plot\n",
        "#sns.distplot(df_train[\"Bookings\"]) ##Distribution Plot\n",
        "\n",
        "\n",
        "#sns.set(font_scale=1.25) \n",
        "\n",
        "#    *hm = sns.heatmap(cm, cbar=True, annot=True, fmt=\".2f\", annot_kws={\"size\":10}, yticklabels=cols.values, xticklabels=cols.values)\n",
        "#   plt.show()**/"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32msnowdenrise\\Data_Analysis_Distribution_Chart\\Data_Analysis_Distribution_Chart.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell://github/snowdenrise/Data_Analysis_Distribution_Chart/Data_Analysis_Distribution_Chart.ipynb#X63sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://github/snowdenrise/Data_Analysis_Distribution_Chart/Data_Analysis_Distribution_Chart.ipynb#X63sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexpress\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpx\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell://github/snowdenrise/Data_Analysis_Distribution_Chart/Data_Analysis_Distribution_Chart.ipynb#X63sdnNjb2RlLXZmcw%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mipywidgets\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mwidgets\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Load the data\n",
        "url = 'https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Define the widgets\n",
        "year_slider = widgets.IntSlider(value=df['year'].min(), min=df['year'].min(), max=df['year'].max(), step=5, description='Year')\n",
        "continent_dropdown = widgets.Dropdown(options=['All'] + list(df['continent'].unique()), value='All', description='Continent')\n",
        "submit_button = widgets.Button(description='Submit')\n",
        "\n",
        "# Define the function to update the plot\n",
        "def update_plot(b):\n",
        "    if continent_dropdown.value == 'All':\n",
        "        filtered_df = df[df['year'] == year_slider.value]\n",
        "    else:\n",
        "        filtered_df = df[(df['year'] == year_slider.value) & (df['continent'] == continent_dropdown.value)]\n",
        "    \n",
        "    fig = px.scatter(filtered_df, x='gdpPercap', y='lifeExp', size='pop', color='continent', log_x=True, range_x=[100, 100000], range_y=[20, 90], height=500)\n",
        "    fig.update_layout(title=str(year_slider.value))\n",
        "    fig.show()\n",
        "\n",
        "# Register the update function to the submit button\n",
        "submit_button.on_click(update_plot)\n",
        "\n",
        "# Display the widgets and the plot\n",
        "display(year_slider)\n",
        "display(continent_dropdown)\n",
        "display(submit_button)\n",
        "\n",
        "update_plot(None)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "J6wpL84HuX3Z"
      ],
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "515ab5237b3b2b704a65bb16665df991990f8d9b97e10d580243abdf3d748bec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
